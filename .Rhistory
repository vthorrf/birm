return(Modelout)
}
Model <- compiler::cmpfun(Model)
Initial.Values <- GIV(Model, MyData, PGF=T)
is.model(Model, Initial.Values, MyData)
is.bayesian(Model, Initial.Values, MyData)
LLfn
## Prior parameters
theta <- interval(parm[Data$pos.theta], -4, 4)
parm[Data$pos.theta] <- theta
kappa <- tanh(parm[Data$pos.kappa])
parm[Data$pos.kappa] <- kappa
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
Data <- MyData
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
basis
### Start====
#require(LaplacesDemon)
#require(compiler)
#require(parallel)
#require(tidyr)
CPUs = detectCores(all.tests = FALSE, logical = TRUE) - 1
if(CPUs == 0) CPUs = 1
### Convert data to long format====
lonlong <- gather(data.frame(x), "item", "resp", colnames(x), factor_key=TRUE)
data_long <- data.frame(ID=rep(1:nrow(x), times=ncol(x)),lonlong)
if (is.null(levels)) {
base_scor <- rowSums(x) / (max(x) * ncol(x))
} else base_scor <- rowSums(x) / ((levels-1) * ncol(x))
if (is.null(knots)) knots <- unique(sort(qtrunc(base_scor, "norm", a=-4, b=4)))
if (is.null(err)) {
err <- rep(1e-4,nrow(x))
} else if(length(err) == nrow(x)) {
err <- err
} else stop("Error estimates for theta (err) are probably missing or are too many")
### Assemble data list====
if (method == "MAP") {
mon.names  <- "LL"
} else { mon.names  <- "LP" }
if(basis=="legendre") {
parm.names <- as.parm.names(list( kappa=rep(0,(degree+2) * ncol(x)),
theta=rep(0, nrow(x)) ))
} else if(basis=="rademacher") {
parm.names <- as.parm.names(list( kappa=rep(0,length(knots)*ncol(x)),
theta=rep(0, nrow(x))  ))
} else if(basis=="bs") {
parm.names <- as.parm.names(list( kappa=rep(0,(length(knots)+
degree)*ncol(x)),
theta=rep(0, nrow(x)) ))
} else if(basis=="nn"){
parm.names <- as.parm.names(list( kappa=rep(0, M * ncol(x) * 2),
theta=rep(0, nrow(x)) ))
}else stop("Unknow basis type :/")
K          <- length(grep("kappa", parm.names))
pos.kappa  <- grep("kappa", parm.names)
pos.theta  <- grep("theta", parm.names)
PGF <- function(Data) {
kappa <- rlaplace(Data$K, 0, 1)
theta <- rtrunc(Data$n, "norm", -4, 4, mean=Data$SS, sd=Data$err)
return(c(kappa, theta))
}
LLfn   <- function(theta, kappa, knots, degree, n, v, SS, K, M) {
if(basis=="legendre") {
### Legendre Orthogonal Polynomials basis expansion
kLL   <- matrix(rep(kappa,each=n),ncol=degree+2)
pol   <- matrix(rep(poly(theta, degree=degree), times=v),
ncol=degree, nrow=n * v)
poll  <- cbind(1,theta,pol)
W       <- rowSums( kLL * poll )
} else if(basis=="rademacher") {
### Rademacher basis expansion
kLL   <- matrix(rep(kappa,each=n),ncol=length(knots))
thetaSS <- rep(theta, times=v)
thetaLL <- matrix(rep(thetaSS, times=length(knots)),
ncol=length(knots))
RadBas  <- sign(sweep(thetaLL,2,knots)) * kLL
W       <- rowSums(RadBas)
} else if(basis=="bs") {
### B-spline basis expansion
kLL   <- matrix(rep(kappa,each=n),ncol=(length(knots)+
degree))
thetaSS <- splines::bs(theta + SS, knots=knots,
degree=degree)
BSP  <- matrix(rep(thetaSS, times=v),
ncol=degree + length(knots),
nrow=n * v, byrow=T)
W       <- rowSums( kLL *BSP )
} else if(basis=="nn"){
### Neural Network
inLL  <- matrix(rep(kappa[1:(K/2)],each=n),ncol=M)
pol   <- matrix(rep(theta, times=v), ncol=M,
nrow=n * v)
otLL  <- matrix(rep(kappa[((K/2)+1):K],each=n),ncol=M)
W       <- rowSums( plogis(inLL * pol) * otLL )
}else stop("Unknow basis type :/")
return(W)
}
MyData <- list(parm.names=parm.names, mon.names=mon.names,
PGF=PGF, X=data_long, n=nrow(x), v=ncol(x),
pos.kappa=pos.kappa, pos.theta=pos.theta, K=K,
SS=qtrunc(base_scor, "norm", a=-4, b=4), M=M,
knots=knots, degree=degree, err=err)
is.data(MyData)
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
require(birm)
rm(list=ls())
dev.off()
cat("\014")
levels=NULL; M=5; basis="bs"; err=NULL; knots=NULL;
data <- simData(100,10)
x <- data$data
levels=NULL; M=5; basis="bs"; err=NULL; knots=NULL;
degree=3; method="VB"; Iters=500; Smpl=1000; Thin=1; A=500;
temp=1e-2; tmax=1; algo="SANN"; seed=666
require(LaplacesDemon)
require(compiler)
require(parallel)
require(tidyr)
knots <- seq(-4,4,len=5)
### Start====
#require(LaplacesDemon)
#require(compiler)
#require(parallel)
#require(tidyr)
CPUs = detectCores(all.tests = FALSE, logical = TRUE) - 1
if(CPUs == 0) CPUs = 1
### Convert data to long format====
lonlong <- gather(data.frame(x), "item", "resp", colnames(x), factor_key=TRUE)
data_long <- data.frame(ID=rep(1:nrow(x), times=ncol(x)),lonlong)
if (is.null(levels)) {
base_scor <- rowSums(x) / (max(x) * ncol(x))
} else base_scor <- rowSums(x) / ((levels-1) * ncol(x))
if (is.null(knots)) knots <- unique(sort(qtrunc(base_scor, "norm", a=-4, b=4)))
if (is.null(err)) {
err <- rep(1e-4,nrow(x))
} else if(length(err) == nrow(x)) {
err <- err
} else stop("Error estimates for theta (err) are probably missing or are too many")
### Assemble data list====
if (method == "MAP") {
mon.names  <- "LL"
} else { mon.names  <- "LP" }
if(basis=="legendre") {
parm.names <- as.parm.names(list( kappa=rep(0,(degree+2) * ncol(x)),
theta=rep(0, nrow(x)) ))
} else if(basis=="rademacher") {
parm.names <- as.parm.names(list( kappa=rep(0,length(knots)*ncol(x)),
theta=rep(0, nrow(x))  ))
} else if(basis=="bs") {
parm.names <- as.parm.names(list( kappa=rep(0,(length(knots)+
degree)*ncol(x)),
theta=rep(0, nrow(x)) ))
} else if(basis=="nn"){
parm.names <- as.parm.names(list( kappa=rep(0, M * ncol(x) * 2),
theta=rep(0, nrow(x)) ))
}else stop("Unknow basis type :/")
K          <- length(grep("kappa", parm.names))
pos.kappa  <- grep("kappa", parm.names)
pos.theta  <- grep("theta", parm.names)
PGF <- function(Data) {
kappa <- rlaplace(Data$K, 0, 1)
theta <- rtrunc(Data$n, "norm", -4, 4, mean=Data$SS, sd=Data$err)
return(c(kappa, theta))
}
LLfn   <- function(theta, kappa, knots, degree, n, v, SS, K, M) {
if(basis=="legendre") {
### Legendre Orthogonal Polynomials basis expansion
kLL   <- matrix(rep(kappa,each=n),ncol=degree+2)
pol   <- matrix(rep(poly(theta, degree=degree), times=v),
ncol=degree, nrow=n * v)
poll  <- cbind(1,theta,pol)
W       <- rowSums( kLL * poll )
} else if(basis=="rademacher") {
### Rademacher basis expansion
kLL   <- matrix(rep(kappa,each=n),ncol=length(knots))
thetaSS <- rep(theta, times=v)
thetaLL <- matrix(rep(thetaSS, times=length(knots)),
ncol=length(knots))
RadBas  <- sign(sweep(thetaLL,2,knots)) * kLL
W       <- rowSums(RadBas)
} else if(basis=="bs") {
### B-spline basis expansion
kLL   <- matrix(rep(kappa,each=n),ncol=(length(knots)+
degree))
thetaSS <- splines::bs(theta + SS, knots=knots,
degree=degree)
BSP  <- matrix(rep(thetaSS, times=v),
ncol=degree + length(knots),
nrow=n * v, byrow=T)
W       <- rowSums( kLL *BSP )
} else if(basis=="nn"){
### Neural Network
inLL  <- matrix(rep(kappa[1:(K/2)],each=n),ncol=M)
pol   <- matrix(rep(theta, times=v), ncol=M,
nrow=n * v)
otLL  <- matrix(rep(kappa[((K/2)+1):K],each=n),ncol=M)
W       <- rowSums( plogis(inLL * pol) * otLL )
}else stop("Unknow basis type :/")
return(W)
}
MyData <- list(parm.names=parm.names, mon.names=mon.names,
PGF=PGF, X=data_long, n=nrow(x), v=ncol(x),
pos.kappa=pos.kappa, pos.theta=pos.theta, K=K,
SS=qtrunc(base_scor, "norm", a=-4, b=4), M=M,
knots=knots, degree=degree, err=err)
is.data(MyData)
theta <- interval(rnorm(MyData$n), -4, 4)
kappa <- tanh(rlaplace(MyData$K))
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
Data <- MyData
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
Lpp
theta.prior
kapp.prior
kappa.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
IRF     <- 1 / ( 1 + exp(-W) )
LL      <- sum( dbinom(Data$X[,3], size=max(Data$X[,3]), prob=IRF, log=T) )
LL
hist(theta)
hist(W)
?laplace
?rlaplace
Model <- function(parm, Data){
## Prior parameters
theta <- interval(parm[Data$pos.theta], -4, 4)
parm[Data$pos.theta] <- theta
kappa <- parm[Data$pos.kappa]
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dlaplace(kappa, location=0, scale=1, log=T))
Lpp <- kappa.prior + theta.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
IRF     <- 1 / ( 1 + exp(-W) )
LL      <- sum( dbinom(Data$X[,3], size=max(Data$X[,3]), prob=IRF, log=T) )
### Log-Posterior
LP <- LL + Lpp
### Estimates
yhat <- qbinom(rep(.5, length(IRF)), size=max(Data$X[,3]), prob=IRF)
### Output
Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yhat, parm=parm)
return(Modelout)
}
Model <- compiler::cmpfun(Model)
Initial.Values <- GIV(Model, MyData, PGF=T)
is.model(Model, Initial.Values, MyData)
is.bayesian(Model, Initial.Values, MyData)
Model <- function(parm, Data){
## Prior parameters
theta <- interval(parm[Data$pos.theta], -4, 4)
parm[Data$pos.theta] <- theta
kappa <- interval(parm[Data$pos.kappa], -1, 1)
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
IRF     <- 1 / ( 1 + exp(-W) )
LL      <- sum( dbinom(Data$X[,3], size=max(Data$X[,3]), prob=IRF, log=T) )
### Log-Posterior
LP <- LL + Lpp
### Estimates
yhat <- qbinom(rep(.5, length(IRF)), size=max(Data$X[,3]), prob=IRF)
### Output
Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yhat, parm=parm)
return(Modelout)
}
Model <- compiler::cmpfun(Model)
Initial.Values <- GIV(Model, MyData, PGF=T)
is.model(Model, Initial.Values, MyData)
is.bayesian(Model, Initial.Values, MyData)
Model <- function(parm, Data){
## Prior parameters
theta <- interval(parm[Data$pos.theta], -4, 4)
parm[Data$pos.theta] <- theta
kappa <- interval(parm[Data$pos.kappa], -1, 1)
parm[Data$pos.kappa] <- kappa
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
IRF     <- 1 / ( 1 + exp(-W) )
LL      <- sum( dbinom(Data$X[,3], size=max(Data$X[,3]), prob=IRF, log=T) )
### Log-Posterior
LP <- LL + Lpp
### Estimates
yhat <- qbinom(rep(.5, length(IRF)), size=max(Data$X[,3]), prob=IRF)
### Output
Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yhat, parm=parm)
return(Modelout)
}
Model <- compiler::cmpfun(Model)
Initial.Values <- GIV(Model, MyData, PGF=T)
is.model(Model, Initial.Values, MyData)
is.bayesian(Model, Initial.Values, MyData)
Model <- function(parm, Data){
## Prior parameters
theta <- interval(parm[Data$pos.theta], -4, 4)
parm[Data$pos.theta] <- theta
kappa <- interval(parm[Data$pos.kappa], -1, 1)
#parm[Data$pos.kappa] <- kappa
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
IRF     <- 1 / ( 1 + exp(-W) )
LL      <- sum( dbinom(Data$X[,3], size=max(Data$X[,3]), prob=IRF, log=T) )
### Log-Posterior
LP <- LL + Lpp
### Estimates
yhat <- qbinom(rep(.5, length(IRF)), size=max(Data$X[,3]), prob=IRF)
### Output
Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yhat, parm=parm)
return(Modelout)
}
Model <- compiler::cmpfun(Model)
Initial.Values <- GIV(Model, MyData, PGF=T)
is.model(Model, Initial.Values, MyData)
is.bayesian(Model, Initial.Values, MyData)
Model <- function(parm, Data){
## Prior parameters
theta <- interval(parm[Data$pos.theta], -4, 4)
parm[Data$pos.theta] <- theta
kappa <- tanh(parm[Data$pos.kappa])
#parm[Data$pos.kappa] <- kappa
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
IRF     <- 1 / ( 1 + exp(-W) )
LL      <- sum( dbinom(Data$X[,3], size=max(Data$X[,3]), prob=IRF, log=T) )
### Log-Posterior
LP <- LL + Lpp
### Estimates
yhat <- qbinom(rep(.5, length(IRF)), size=max(Data$X[,3]), prob=IRF)
### Output
Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yhat, parm=parm)
return(Modelout)
}
Model <- compiler::cmpfun(Model)
Initial.Values <- GIV(Model, MyData, PGF=T)
is.model(Model, Initial.Values, MyData)
is.bayesian(Model, Initial.Values, MyData)
Model <- function(parm, Data){
## Prior parameters
theta <- interval(parm[Data$pos.theta], -4, 4)
parm[Data$pos.theta] <- theta
kappa <- tanh(parm[Data$pos.kappa])
parm[Data$pos.kappa] <- kappa
### Log-Priors
theta.prior <- sum(dtrunc(theta, "norm",    -4, 4, mean=Data$SS,
sd=Data$err, log=T))
kappa.prior <- sum(dtrunc(kappa, "laplace", -1, 1, location=0,
scale=1,     log=T))
Lpp <- kappa.prior + theta.prior
### Log-Likelihood
W       <- LLfn(theta, kappa, Data$knots, Data$degree,
Data$n, Data$v, Data$SS, Data$K, Data$M)
IRF     <- 1 / ( 1 + exp(-W) )
LL      <- sum( dbinom(Data$X[,3], size=max(Data$X[,3]), prob=IRF, log=T) )
### Log-Posterior
LP <- LL + Lpp
### Estimates
yhat <- qbinom(rep(.5, length(IRF)), size=max(Data$X[,3]), prob=IRF)
### Output
Modelout <- list(LP=LP, Dev=-2*LL, Monitor=LP, yhat=yhat, parm=parm)
return(Modelout)
}
Model <- compiler::cmpfun(Model)
Initial.Values <- GIV(Model, MyData, PGF=T)
is.model(Model, Initial.Values, MyData)
is.bayesian(Model, Initial.Values, MyData)
require(birm)
rm(list=ls())
dev.off()
cat("\014")
plot(tanh(seq(-10,10,len=1000)) ~ seq(-10,10,len=1000), type="l")
require(birm)
rm(list=ls())
dev.off()
cat("\014")
library(birm)
require(birm)
rm(list=ls())
dev.off()
cat("\014")
data <- simData(100,10)
x <- data$data
G1 <- optscr(x, basis="bs", method="MAP", Iters=10000)
G2 <- rasch(x, method="MAP", Iters=10000)
cor(cbind(G1$abil, G2$abil, rowMeans(x), data$abil), method="s")
plot(data.frame(G1$abil, G2$abil, rowMeans(x), data$abil))
G1$FitIndexes$SABIC; G2$FitIndexes$SABIC
require(birm)
rm(list=ls())
dev.off()
cat("\014")
data <- simData(100,10)
x <- data$data
G1 <- optscr(x, basis="bs", knots=seq(-4,4,len=5), method="MAP", Iters=10000)
G2 <- rasch(x, method="MAP", Iters=10000)
cor(cbind(G1$abil, G2$abil, rowMeans(x), data$abil), method="s")
plot(data.frame(G1$abil, G2$abil, rowMeans(x), data$abil))
G1$FitIndexes$SABIC; G2$FitIndexes$SABIC
#G1$DIC$DIC; G2$DIC$DIC
#levels=NULL; M=5; basis="bs"; err=NULL; knots=NULL;
#degree=3; method="VB"; Iters=500; Smpl=1000; Thin=1; A=500;
#temp=1e-2; tmax=1; algo="SANN"; seed=666
#require(LaplacesDemon)
#require(compiler)
#require(parallel)
#require(tidyr)
require(birm)
rm(list=ls())
dev.off()
cat("\014")
data <- simData(100,10)
x <- data$data
G1 <- optscr(x, basis="bs", knots=seq(-4,4,len=7), method="MAP", Iters=10000)
G2 <- rasch(x, method="MAP", Iters=10000)
cor(cbind(G1$abil, G2$abil, rowMeans(x), data$abil), method="s")
plot(data.frame(G1$abil, G2$abil, rowMeans(x), data$abil))
G1$FitIndexes$SABIC; G2$FitIndexes$SABIC
#G1$DIC$DIC; G2$DIC$DIC
#levels=NULL; M=5; basis="bs"; err=NULL; knots=NULL;
#degree=3; method="VB"; Iters=500; Smpl=1000; Thin=1; A=500;
#temp=1e-2; tmax=1; algo="SANN"; seed=666
#require(LaplacesDemon)
#require(compiler)
#require(parallel)
#require(tidyr)
require(birm)
rm(list=ls())
dev.off()
cat("\014")
data <- simData(100,10)
x <- data$data
G1 <- optscr(x, basis="bs", knots=seq(-4,4,len=7), method="LA", Iters=10000)
G2 <- rasch(x, method="LA", Iters=10000)
cor(cbind(G1$abil, G2$abil, rowMeans(x), data$abil), method="s")
plot(data.frame(G1$abil, G2$abil, rowMeans(x), data$abil))
G1$FitIndexes$SABIC; G2$FitIndexes$SABIC
#G1$DIC$DIC; G2$DIC$DIC
#levels=NULL; M=5; basis="bs"; err=NULL; knots=NULL;
#degree=3; method="VB"; Iters=500; Smpl=1000; Thin=1; A=500;
#temp=1e-2; tmax=1; algo="SANN"; seed=666
#require(LaplacesDemon)
#require(compiler)
#require(parallel)
#require(tidyr)
G1 <- optscr(x, basis="nn", knots=seq(-4,4,len=7), method="LA", Iters=10000)
G1 <- optscr(x, basis="nn", knots=seq(-4,4,len=7), method="LA", Iters=2000)
G1 <- optscr(x, basis="nn", M=5, method="LA", Iters=2000)
library(birm)
library(birm)
library(birm)
library(birm)
library(birm)
library(birm)
library(birm)
